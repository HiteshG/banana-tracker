{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BananaTracker - Multi-Object Tracking with SAM2.1 + Cutie\n",
    "\n",
    "This notebook demonstrates the complete MOT tracking pipeline using:\n",
    "- **YOLOv8** for object detection\n",
    "- **ByteTrack-based tracker** for multi-object tracking\n",
    "- **SAM2.1** for high-quality mask generation from bounding boxes\n",
    "- **Cutie** for temporal mask propagation\n",
    "\n",
    "The mask module enhances tracking by providing pixel-level precision that improves association when objects are close together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 1: Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install ultralytics opencv-python-headless tqdm\n",
    "!pip install lap cython_bbox  # For ByteTrack tracker core\n",
    "\n",
    "# Install SAM2.1 dependencies (HuggingFace transformers)\n",
    "!pip install transformers>=4.35.0 huggingface_hub\n",
    "\n",
    "# Install Cutie dependencies\n",
    "!pip install omegaconf hydra-core\n",
    "\n",
    "# GTA-Link post-processing dependencies\n",
    "!pip install scikit-learn loguru seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 2: Clone Repositories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Clone BananaTracker repository\n",
    "if not os.path.exists('bananatracker'):\n",
    "    !git clone https://github.com/USER/bananatracker.git\n",
    "\n",
    "# Clone Cutie for temporal mask propagation\n",
    "if not os.path.exists('Cutie'):\n",
    "    !git clone https://github.com/hkchengrex/Cutie.git\n",
    "\n",
    "# Create symlink for Cutie in mask_propagation folder\n",
    "os.makedirs('bananatracker/bananatracker/mask_propagation', exist_ok=True)\n",
    "if not os.path.exists('bananatracker/bananatracker/mask_propagation/Cutie'):\n",
    "    os.symlink('/content/Cutie', 'bananatracker/bananatracker/mask_propagation/Cutie')\n",
    "\n",
    "# Download Cutie weights\n",
    "os.makedirs('Cutie/weights', exist_ok=True)\n",
    "if not os.path.exists('Cutie/weights/cutie-base-mega.pth'):\n",
    "    !wget -P Cutie/weights https://github.com/hkchengrex/Cutie/releases/download/v1.0/cutie-base-mega.pth\n",
    "\n",
    "# Install BananaTracker in development mode\n",
    "%cd bananatracker\n",
    "!pip install -e .\n",
    "%cd ..\n",
    "\n",
    "# Clone GTA-Link for post-processing track refinement\n",
    "if not os.path.exists('/content/gta-link'):\n",
    "    !git clone https://github.com/sjc042/gta-link.git /content/gta-link\n",
    "\n",
    "# Make torchreid importable via PYTHONPATH (no setup.py needed — rank_cy Cython\n",
    "# extension is eval-only; FeatureExtractor used by GTA-Link is pure Python).\n",
    "os.environ['PYTHONPATH'] = '/content/gta-link/reid:' + os.environ.get('PYTHONPATH', '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 3: SAM2.1 Model Configuration\n",
    "\n",
    "Configure SAM2.1 model settings. You can choose different model sizes:\n",
    "- `facebook/sam2.1-hiera-tiny` - Fastest, lower quality\n",
    "- `facebook/sam2.1-hiera-small` - Good balance\n",
    "- `facebook/sam2.1-hiera-base-plus` - Better quality\n",
    "- `facebook/sam2.1-hiera-large` - Best quality (recommended)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title SAM2.1 Configuration { display-mode: \"form\" }\n",
    "\n",
    "# SAM2.1 Model Settings\n",
    "SAM2_MODEL_ID = \"facebook/sam2.1-hiera-large\"  #@param [\"facebook/sam2.1-hiera-tiny\", \"facebook/sam2.1-hiera-small\", \"facebook/sam2.1-hiera-base-plus\", \"facebook/sam2.1-hiera-large\"]\n",
    "SAM2_CHECKPOINT = \"\"  #@param {type:\"string\"} # Optional: local checkpoint path (leave empty to use HuggingFace)\n",
    "\n",
    "# HuggingFace Token (required for gated models, optional for SAM2)\n",
    "HF_TOKEN = \"\"  #@param {type:\"string\"}\n",
    "\n",
    "# Cutie Weights Path\n",
    "CUTIE_WEIGHTS = \"/content/Cutie/weights/cutie-base-mega.pth\"  #@param {type:\"string\"}\n",
    "\n",
    "print(f\"SAM2.1 Model: {SAM2_MODEL_ID}\")\n",
    "print(f\"SAM2.1 Checkpoint: {SAM2_CHECKPOINT if SAM2_CHECKPOINT else 'Using HuggingFace download'}\")\n",
    "print(f\"HF Token: {'Set' if HF_TOKEN else 'Not set (optional)'}\")\n",
    "print(f\"Cutie Weights: {CUTIE_WEIGHTS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 4: Tracker Configuration\n",
    "\n",
    "Configure the full tracking pipeline with detection, tracking, and mask settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import sys\nsys.path.insert(0, '/content/bananatracker')\nsys.path.insert(0, '/content/Cutie')\n\nfrom bananatracker import BananaTrackerConfig\n\n# Full configuration for hockey/sports tracking with mask enhancement\n# NOTE: These values are optimized for better tracking performance:\n#   - Lower detection thresholds to catch more objects\n#   - Larger track_buffer for longer occlusion handling (3 seconds)\n#   - Lower track_thresh for more lenient first-pass matching\nconfig = BananaTrackerConfig(\n    # Detection Settings\n    yolo_weights=\"/content/HockeyAI_model_weight.pt\",  # Update with your model path\n    class_names=[\"Center Ice\", \"Faceoff\", \"Goalpost\", \"Goaltender\", \"Player\", \"Puck\", \"Referee\"],\n    track_classes=[3, 4, 5, 6],  # Goaltender, Player, Puck, Referee\n    special_classes=[5],          # Puck - max-conf only\n    detection_conf_thresh=0.4,    # Lowered to catch more objects\n    detection_iou_thresh=0.7,     # IoU threshold for YOLO NMS\n\n    # Post-processing: Centroid-based deduplication\n    centroid_dedup_enabled=True,\n    centroid_dedup_max_distance=36,\n\n    # Tracker Settings (ByteTrack) - Optimized for stability\n    track_thresh=0.5,    # Lowered for more first-pass matches\n    track_buffer=90,     # 3 seconds at 30fps for better occlusion handling\n    match_thresh=0.8,\n    fps=30,\n    cmc_method=\"orb\",  # Camera motion compensation\n\n    # Mask Module Settings (SAM2.1 + Cutie)\n    enable_masks=True,\n    sam2_model_id=SAM2_MODEL_ID,\n    sam2_checkpoint=SAM2_CHECKPOINT if SAM2_CHECKPOINT else None,\n    cutie_weights_path=CUTIE_WEIGHTS,\n    hf_token=HF_TOKEN if HF_TOKEN else None,\n    mask_start_frame=1,\n    mask_bbox_overlap_threshold=0.6,\n\n    # Visualization Settings\n    class_colors={\n        \"Goaltender\": (255, 165, 0),   # Orange\n        \"Player\": (255, 0, 0),          # Blue (BGR)\n        \"Puck\": (0, 255, 0),            # Green\n        \"Referee\": (0, 0, 255),         # Red\n    },\n    show_track_id=True,\n    show_masks=True,     # Enable mask overlay in visualization\n    mask_alpha=0.5,      # Mask transparency\n    line_thickness=2,\n\n    # Output Settings\n    output_video_path=\"/content/output_tracked_with_masks.mp4\",\n    output_txt_path=\"/content/results.txt\",\n    device=\"cuda:0\",\n)\n\nprint(\"Configuration created!\")\nprint(f\"\\nDetection:\")\nprint(f\"  - Confidence threshold: {config.detection_conf_thresh}\")\nprint(f\"  - Tracking classes: {config.track_classes}\")\nprint(f\"\\nTracker:\")\nprint(f\"  - Track threshold: {config.track_thresh}\")\nprint(f\"  - Track buffer: {config.track_buffer} frames ({config.track_buffer/config.fps:.1f}s)\")\nprint(f\"\\nMask Module:\")\nprint(f\"  - Enabled: {config.enable_masks}\")\nprint(f\"  - SAM2.1 Model: {config.sam2_model_id}\")\nprint(f\"  - Cutie Weights: {config.cutie_weights_path}\")\nprint(f\"\\nVisualization:\")\nprint(f\"  - Show masks: {config.show_masks}\")\nprint(f\"  - Mask alpha: {config.mask_alpha}\")"
  },
  {
   "cell_type": "markdown",
   "source": "## Cell 4b: Enable Debug Logging (Optional)\n\nEnable verbose debug logging to track LOST, REMOVED, CREATED, and REJECTED events during tracking. Useful for diagnosing ID switches and track loss issues.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "#@title Debug Logging Configuration { display-mode: \"form\" }\nimport logging\n\n# Enable/disable debug logging\nENABLE_DEBUG_LOGGING = True  #@param {type:\"boolean\"}\n\n# Configure the BananaTracker logger\nlogger = logging.getLogger(\"BananaTracker\")\n\nif ENABLE_DEBUG_LOGGING:\n    # Set up console handler with detailed formatting\n    handler = logging.StreamHandler()\n    handler.setLevel(logging.DEBUG)\n    formatter = logging.Formatter(\n        '%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n        datefmt='%H:%M:%S'\n    )\n    handler.setFormatter(formatter)\n    \n    # Clear existing handlers and add new one\n    logger.handlers.clear()\n    logger.addHandler(handler)\n    logger.setLevel(logging.DEBUG)\n    \n    print(\"Debug logging ENABLED\")\n    print(\"  - Track LOST: When a track loses its detection\")\n    print(\"  - Track REMOVED: When an unconfirmed track exceeds grace period\")\n    print(\"  - Track CREATED: When a new track is initialized\")\n    print(\"  - Detection REJECTED: When detection confidence is below threshold\")\n    print(\"\\nLogs will appear during pipeline.process_video() execution.\")\nelse:\n    logger.setLevel(logging.WARNING)\n    print(\"Debug logging DISABLED (only warnings and errors will be shown)\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 5: Initialize Pipeline with SAM2.1 + Cutie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bananatracker import BananaTrackerPipeline\n",
    "\n",
    "# Initialize the pipeline (this will load SAM2.1 and Cutie models)\n",
    "print(\"Initializing pipeline...\")\n",
    "print(\"This may take a moment to download SAM2.1 model from HuggingFace...\")\n",
    "\n",
    "pipeline = BananaTrackerPipeline(config)\n",
    "\n",
    "print(\"\\nPipeline initialized!\")\n",
    "print(f\"  - Detector: YOLOv8\")\n",
    "print(f\"  - Tracker: BananaTracker (ByteTrack-based with mask enhancement)\")\n",
    "print(f\"  - Mask Module: {'SAM2.1 + Cutie' if pipeline.mask_manager else 'Disabled'}\")\n",
    "print(f\"  - CMC Method: {config.cmc_method}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 6: Validate SAM2.1 Mask Generation (Test Cell)\n",
    "\n",
    "This cell validates that SAM2.1 is working correctly by generating masks on a random frame from the video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport random\n\n# Input video for testing\nTEST_VIDEO = \"/content/sample_video.mp4\"  # Update with your video path\n\n# Open video and get a random frame\ncap = cv2.VideoCapture(TEST_VIDEO)\nif not cap.isOpened():\n    print(f\"Error: Could not open video {TEST_VIDEO}\")\nelse:\n    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n    random_frame_idx = random.randint(0, total_frames - 1)\n    \n    cap.set(cv2.CAP_PROP_POS_FRAMES, random_frame_idx)\n    ret, frame = cap.read()\n    cap.release()\n    \n    if ret:\n        print(f\"Testing SAM2.1 on frame {random_frame_idx} of {total_frames}\")\n        \n        # Convert to RGB for SAM2.1\n        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n        \n        # Run detection to get bounding boxes\n        detections = pipeline.detector.detect(frame)\n        \n        if len(detections) > 0:\n            # Get bounding boxes (first 4 columns: x1, y1, x2, y2)\n            boxes_xyxy = detections[:, :4].tolist()\n            \n            print(f\"Detected {len(boxes_xyxy)} objects\")\n            \n            # Generate masks using SAM2.1\n            if pipeline.mask_manager is not None:\n                print(\"Generating masks with SAM2.1...\")\n                try:\n                    masks = pipeline.mask_manager._sam2_predict_boxes(frame_rgb, boxes_xyxy)\n                    \n                    print(f\"Generated {len(masks)} masks\")\n                    print(f\"Mask shape: {masks.shape}\")\n                    \n                    # Visualize results\n                    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n                    \n                    # Original frame\n                    axes[0].imshow(frame_rgb)\n                    axes[0].set_title(f'Original Frame #{random_frame_idx}')\n                    axes[0].axis('off')\n                    \n                    # Frame with bounding boxes\n                    frame_with_boxes = frame_rgb.copy()\n                    for i, box in enumerate(boxes_xyxy):\n                        x1, y1, x2, y2 = map(int, box)\n                        cv2.rectangle(frame_with_boxes, (x1, y1), (x2, y2), (255, 0, 0), 2)\n                        cv2.putText(frame_with_boxes, f'{i}', (x1, y1-10), \n                                    cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 0, 0), 2)\n                    axes[1].imshow(frame_with_boxes)\n                    axes[1].set_title(f'Detected Objects ({len(boxes_xyxy)})')\n                    axes[1].axis('off')\n                    \n                    # Combined masks overlay\n                    colors = plt.cm.tab10(np.linspace(0, 1, 10))[:, :3] * 255\n                    combined_mask = np.zeros_like(frame_rgb, dtype=np.float32)\n                    for i, mask in enumerate(masks):\n                        color = colors[i % len(colors)]\n                        mask_3d = np.stack([mask, mask, mask], axis=-1).astype(np.float32)\n                        combined_mask += mask_3d * color / 255\n                    \n                    alpha = 0.5\n                    frame_with_masks = frame_rgb.astype(np.float32)\n                    mask_overlay = np.clip(combined_mask, 0, 255)\n                    binary_mask = np.any(combined_mask > 0, axis=-1, keepdims=True)\n                    frame_with_masks = np.where(\n                        binary_mask,\n                        frame_with_masks * alpha + mask_overlay * (1 - alpha),\n                        frame_with_masks\n                    )\n                    axes[2].imshow(frame_with_masks.astype(np.uint8))\n                    axes[2].set_title('SAM2.1 Mask Overlay')\n                    axes[2].axis('off')\n                    \n                    plt.tight_layout()\n                    plt.savefig('/content/sam2_mask_validation.png', dpi=150, bbox_inches='tight')\n                    plt.show()\n                    \n                    print(\"\\n✓ SAM2.1 mask generation validated successfully!\")\n                    print(f\"  Validation image saved to: /content/sam2_mask_validation.png\")\n                    \n                except Exception as e:\n                    print(f\"Error during mask generation: {e}\")\n                    import traceback\n                    traceback.print_exc()\n            else:\n                print(\"Warning: Mask module not initialized\")\n        else:\n            print(\"No objects detected in this frame. Try a different frame.\")\n    else:\n        print(f\"Error: Could not read frame {random_frame_idx}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 7: Run Full Tracking Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to input video\n",
    "INPUT_VIDEO = \"/content/sample_video.mp4\"  # Update with your video path\n",
    "\n",
    "# Run tracking with mask enhancement\n",
    "print(f\"Processing video: {INPUT_VIDEO}\")\n",
    "print(f\"Mask module: {'Enabled' if config.enable_masks else 'Disabled'}\")\n",
    "print(\"\")\n",
    "\n",
    "all_tracks = pipeline.process_video(INPUT_VIDEO)\n",
    "\n",
    "print(f\"\\nProcessed {len(all_tracks)} frames\")\n",
    "print(f\"Output video: {config.output_video_path}\")\n",
    "print(f\"MOT results: {config.output_txt_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "---\n## GTA-Link Post-Processing\n\nRefine MOT results using GTA-Link's two-phase pipeline:\n1. **Phase 1 — Generate Tracklets**: Extract per-track appearance features using OSNet re-ID model\n2. **Phase 2 — Refine Tracklets**: Split identity switches (DBSCAN clustering) then merge fragmented tracks (hierarchical cosine-distance merging)\n\nOutput: refined MOT `.txt`, re-rendered video with refined tracks, before/after summary statistics.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "#@title GTA-Link Configuration { display-mode: \"form\" }\n",
    "\n",
    "# ─── Master toggle ───────────────────────────────────────────\n",
    "GTA_ENABLED = True  #@param {type:\"boolean\"}\n",
    "\n",
    "# ─── Classes to refine ────────────────────────────────────────\n",
    "# Class IDs (from YOLO model):\n",
    "#   0=Center Ice  1=Faceoff  2=Goalpost  3=Goaltender\n",
    "#   4=Player      5=Puck     6=Referee\n",
    "# Default \"3,4,6\" refines Goaltender, Player, Referee.\n",
    "# Puck (5) is excluded by default — short tracklets confuse the splitter.\n",
    "GTA_CLASSES_TO_REFINE = \"3,4,6\"  #@param {type:\"string\"}\n",
    "\n",
    "# ─── Phase 2 — Split parameters ───────────────────────────────\n",
    "GTA_USE_SPLIT = True   #@param {type:\"boolean\"}\n",
    "GTA_EPS = 0.7          #@param {type:\"number\"}\n",
    "GTA_MIN_SAMPLES = 10   #@param {type:\"number\"}\n",
    "GTA_MAX_K = 3          #@param {type:\"number\"}\n",
    "GTA_MIN_LEN = 100      #@param {type:\"number\"}\n",
    "\n",
    "# ─── Phase 2 — Connect (merge) parameters ────────────────────\n",
    "GTA_USE_CONNECT = True       #@param {type:\"boolean\"}\n",
    "GTA_MERGE_DIST_THRES = 0.4   #@param {type:\"number\"}\n",
    "GTA_SPATIAL_FACTOR = 1.0     #@param {type:\"number\"}\n",
    "\n",
    "# ─── Output-path labels ──────────────────────────────────────\n",
    "GTA_TRACKER_NAME = \"BananaTracker\"  #@param {type:\"string\"}\n",
    "GTA_DATASET_NAME = \"hockey\"         #@param {type:\"string\"}\n",
    "GTA_SEQ_NAME = \"sequence\"           #@param {type:\"string\"}\n",
    "\n",
    "# ─── Derived ──────────────────────────────────────────────────\n",
    "GTA_CLASSES_TO_REFINE_LIST = [int(x.strip()) for x in GTA_CLASSES_TO_REFINE.split(\",\") if x.strip()]\n",
    "\n",
    "print(\"=\" * 52)\n",
    "print(\"GTA-Link Post-Processing Configuration\")\n",
    "print(\"=\" * 52)\n",
    "print(f\"  Enabled            : {GTA_ENABLED}\")\n",
    "print(f\"  Classes to refine  : {GTA_CLASSES_TO_REFINE_LIST}\")\n",
    "print(f\"  Split              : {GTA_USE_SPLIT} (eps={GTA_EPS}, min_samples={GTA_MIN_SAMPLES}, max_k={GTA_MAX_K}, min_len={GTA_MIN_LEN})\")\n",
    "print(f\"  Connect            : {GTA_USE_CONNECT} (merge_dist={GTA_MERGE_DIST_THRES}, spatial={GTA_SPATIAL_FACTOR})\")\n",
    "print(f\"  Tracker / Dataset  : {GTA_TRACKER_NAME} / {GTA_DATASET_NAME}\")\n",
    "print(\"=\" * 52)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1 — Prepare inputs\n",
    "Frees GPU memory held by SAM2.1/Cutie/YOLO, extracts video frames to disk, and writes a class-filtered MOT `.txt` for GTA-Link to consume."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import gc, os, sys\n",
    "import cv2\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ── guard ─────────────────────────────────────────────────────\n",
    "if not GTA_ENABLED:\n",
    "    print(\"GTA_ENABLED is False — skipping Step 1.\")\n",
    "else:\n",
    "    # ── 1. Free GPU ───────────────────────────────────────────\n",
    "    try:\n",
    "        del pipeline\n",
    "    except NameError:\n",
    "        pass\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    print(\"GPU memory freed.\")\n",
    "\n",
    "    # ── 2. Directory layout ───────────────────────────────────\n",
    "    GTA_DATA_PATH  = \"/content/gta_input/frames\"\n",
    "    GTA_PRED_DIR   = \"/content/gta_input/predictions\"\n",
    "    GTA_IMG_DIR    = os.path.join(GTA_DATA_PATH, GTA_SEQ_NAME, \"img1\")\n",
    "    os.makedirs(GTA_IMG_DIR,  exist_ok=True)\n",
    "    os.makedirs(GTA_PRED_DIR, exist_ok=True)\n",
    "    print(f\"Frame dir : {GTA_IMG_DIR}\")\n",
    "    print(f\"Pred dir  : {GTA_PRED_DIR}\")\n",
    "\n",
    "    # ── 3. Extract frames ─────────────────────────────────────\n",
    "    cap = cv2.VideoCapture(INPUT_VIDEO)\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    print(f\"Extracting {total_frames} frames from {INPUT_VIDEO} ...\")\n",
    "    for i in tqdm(range(total_frames), desc=\"Extracting frames\"):\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        cv2.imwrite(os.path.join(GTA_IMG_DIR, f\"{i+1:06d}.jpg\"), frame)\n",
    "    cap.release()\n",
    "    actual_frames = len(os.listdir(GTA_IMG_DIR))\n",
    "    print(f\"Extracted {actual_frames} frames.\")\n",
    "\n",
    "    # ── 4. Build filtered MOT .txt ────────────────────────────\n",
    "    # all_tracks: list of (frame_id, list[STrack])\n",
    "    mot_path = os.path.join(GTA_PRED_DIR, f\"{GTA_SEQ_NAME}.txt\")\n",
    "    REFINED_TRACK_IDS = set()          # original track_ids sent to GTA\n",
    "    TRACK_CLASS_MAP   = {}             # (frame_id, track_id) -> class_id\n",
    "    line_count = 0\n",
    "    with open(mot_path, \"w\") as f:\n",
    "        for frame_id, tracks in all_tracks:\n",
    "            for t in tracks:\n",
    "                if t.class_id not in GTA_CLASSES_TO_REFINE_LIST:\n",
    "                    continue\n",
    "                tlwh = t.tlwh\n",
    "                f.write(f\"{frame_id},{t.track_id},{tlwh[0]:.2f},{tlwh[1]:.2f},\"\n",
    "                        f\"{tlwh[2]:.2f},{tlwh[3]:.2f},{t.score:.2f},-1,-1,-1\\n\")\n",
    "                REFINED_TRACK_IDS.add(t.track_id)\n",
    "                TRACK_CLASS_MAP[(frame_id, t.track_id)] = t.class_id\n",
    "                line_count += 1\n",
    "\n",
    "    print(f\"\\nFiltered MOT .txt written to {mot_path}\")\n",
    "    print(f\"  Lines written        : {line_count}\")\n",
    "    print(f\"  Unique track IDs     : {len(REFINED_TRACK_IDS)}\")\n",
    "    print(f\"  Classes in file      : {sorted(set(TRACK_CLASS_MAP.values()))}\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2 — Phase 1: Generate Tracklets\n",
    "Runs `generate_tracklets.py` via subprocess. This is the expensive step: it performs OSNet re-ID inference on every detection crop across every frame. Subsequent tuning only requires re-running Phase 2 (Step 3)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "import subprocess, glob as _glob\n\nif not GTA_ENABLED:\n    print(\"GTA_ENABLED is False — skipping Phase 1.\")\nelse:\n    OSNET_CKPT = \"/content/gta-link/reid_checkpoints/sports_model.pth.tar-60\"\n\n    # ── pre-flight checks ─────────────────────────────────────\n    if not os.path.isfile(OSNET_CKPT):\n        raise FileNotFoundError(\n            f\"OSNet checkpoint not found: {OSNET_CKPT}\\n\"\n            \"Download it and place it at the path above, or update OSNET_CKPT.\")\n    if not os.path.isfile(os.path.join(\"/content/gta-link\", \"generate_tracklets.py\")):\n        raise FileNotFoundError(\"generate_tracklets.py not found in /content/gta-link\")\n\n    # ── patch torchreid for PyTorch ≥ 2.6 (weights_only default) ──\n    # torch.load changed its default from weights_only=False to True in 2.6;\n    # the bundled torchreid predates that change.  Patch is idempotent.\n    _torchtools = \"/content/gta-link/reid/torchreid/utils/torchtools.py\"\n    with open(_torchtools) as _f:\n        _src = _f.read()\n    if \"weights_only=False\" not in _src:\n        _src = _src.replace(\n            \"torch.load(fpath, map_location=map_location)\",\n            \"torch.load(fpath, map_location=map_location, weights_only=False)\")\n        with open(_torchtools, \"w\") as _f:\n            _f.write(_src)\n        print(\"Patched torchtools.py: added weights_only=False to torch.load\")\n\n    # ── explicit env so torchreid is importable in the child ──\n    env = {**os.environ,\n           \"PYTHONPATH\": \"/content/gta-link/reid:\" + os.environ.get(\"PYTHONPATH\", \"\")}\n\n    cmd = [\n        sys.executable,\n        \"generate_tracklets.py\",\n        \"--model_path\", OSNET_CKPT,\n        \"--data_path\",  GTA_DATA_PATH,\n        \"--pred_dir\",   GTA_PRED_DIR,\n        \"--tracker\",    GTA_TRACKER_NAME,\n    ]\n    print(\"Running Phase 1:\", \" \".join(cmd))\n    result = subprocess.run(cmd, cwd=\"/content/gta-link\", env=env,\n                            stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)\n    print(result.stdout)          # always surface full output\n    if result.returncode != 0:\n        raise RuntimeError(\n            f\"generate_tracklets.py exited with code {result.returncode}\\n\"\n            f\"--- captured output ---\\n{result.stdout}\")\n\n    # ── locate output ─────────────────────────────────────────\n    GTA_TRACKLETS_DIR = os.path.join(\n        os.path.dirname(GTA_PRED_DIR),\n        f\"{GTA_TRACKER_NAME}_Tracklets_{os.path.basename(GTA_DATA_PATH)}\"\n    )\n    pkl_files = _glob.glob(os.path.join(GTA_TRACKLETS_DIR, \"*.pkl\"))\n    print(f\"\\nTracklets dir : {GTA_TRACKLETS_DIR}\")\n    print(f\"  .pkl files  : {pkl_files}\")\n    if not pkl_files:\n        raise FileNotFoundError(f\"No .pkl files found in {GTA_TRACKLETS_DIR}\")\n",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3 — Phase 2: Refine Tracklets\n",
    "Splits identity switches and merges fragmented tracks. This cell can be re-run independently after tuning the split / connect parameters in the config cell above."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "import subprocess, glob as _glob, os\nfrom pathlib import Path\n\nif not GTA_ENABLED:\n    print(\"GTA_ENABLED is False — skipping Phase 2.\")\nelse:\n    env = {**os.environ,\n           \"PYTHONPATH\": \"/content/gta-link/reid:\" + os.environ.get(\"PYTHONPATH\", \"\")}\n\n    cmd = [\n        sys.executable,\n        \"refine_tracklets.py\",\n        \"--dataset\",          GTA_DATASET_NAME,\n        \"--tracker\",          GTA_TRACKER_NAME,\n        \"--track_src\",        GTA_TRACKLETS_DIR,\n        \"--eps\",              str(GTA_EPS),\n        \"--min_samples\",      str(GTA_MIN_SAMPLES),\n        \"--max_k\",            str(GTA_MAX_K),\n        \"--min_len\",          str(GTA_MIN_LEN),\n        \"--merge_dist_thres\", str(GTA_MERGE_DIST_THRES),\n        \"--spatial_factor\",   str(GTA_SPATIAL_FACTOR),\n    ]\n    if GTA_USE_SPLIT:\n        cmd.append(\"--use_split\")\n    if GTA_USE_CONNECT:\n        cmd.append(\"--use_connect\")\n\n    print(\"Running Phase 2:\", \" \".join(cmd))\n    result = subprocess.run(cmd, cwd=\"/content/gta-link\", env=env,\n                            stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)\n    print(result.stdout)          # always surface full output\n    if result.returncode != 0:\n        raise RuntimeError(\n            f\"refine_tracklets.py exited with code {result.returncode}\\n\"\n            f\"--- captured output ---\\n{result.stdout}\")\n\n    # ── locate refined .txt via glob + mtime ─────────────────\n    # Output dir pattern: <parent>/<TRACKER>_<DATASET>_*/<SEQ_NAME>.txt\n    parent = os.path.dirname(GTA_TRACKLETS_DIR)  # /content/gta_input\n    pattern = os.path.join(parent, f\"{GTA_TRACKER_NAME}_{GTA_DATASET_NAME}_*\", f\"{GTA_SEQ_NAME}.txt\")\n    candidates = _glob.glob(pattern)\n    if not candidates:\n        raise FileNotFoundError(f\"No refined .txt matched pattern: {pattern}\")\n    # pick most recently modified\n    GTA_REFINED_TXT = max(candidates, key=os.path.getmtime)\n    print(f\"\\nRefined MOT .txt : {GTA_REFINED_TXT}\")\n",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4 — Merge, Re-render & Summarize\n",
    "Merges refined tracks with passthrough tracks (classes not sent to GTA-Link), re-renders the video with the merged result, and prints a before/after summary."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import cv2\nimport numpy as np\nfrom collections import defaultdict\n\nif not GTA_ENABLED:\n    print(\"GTA_ENABLED is False — skipping Step 4.\")\nelse:\n    # ── class metadata (mirrors config cell) ─────────────────\n    CLASS_NAMES = [\"Center Ice\", \"Faceoff\", \"Goalpost\", \"Goaltender\",\n                   \"Player\", \"Puck\", \"Referee\"]\n    CLASS_COLORS_BGR = {                        # BGR, same as config\n        3: (255, 165,   0),   # Goaltender – orange\n        4: (255,   0,   0),   # Player     – blue\n        5: (  0, 255,   0),   # Puck       – green\n        6: (  0,   0, 255),   # Referee    – red\n    }\n\n    # ── helper: IoU ───────────────────────────────────────────\n    def _iou(box_a, box_b):\n        \"\"\"IoU between two [l, t, w, h] boxes.\"\"\"\n        a_x1, a_y1 = box_a[0], box_a[1]\n        a_x2, a_y2 = a_x1 + box_a[2], a_y1 + box_a[3]\n        b_x1, b_y1 = box_b[0], box_b[1]\n        b_x2, b_y2 = b_x1 + box_b[2], b_y1 + box_b[3]\n        inter_x1 = max(a_x1, b_x1);  inter_y1 = max(a_y1, b_y1)\n        inter_x2 = min(a_x2, b_x2);  inter_y2 = min(a_y2, b_y2)\n        inter   = max(0, inter_x2 - inter_x1) * max(0, inter_y2 - inter_y1)\n        union   = (box_a[2]*box_a[3]) + (box_b[2]*box_b[3]) - inter\n        return inter / union if union > 0 else 0.0\n\n    # ── 1. Parse refined .txt ─────────────────────────────────\n    refined_by_frame = defaultdict(list)   # frame_id -> [(tid, [l,t,w,h])]\n    with open(GTA_REFINED_TXT) as f:\n        for line in f:\n            parts = line.strip().split(\",\")\n            fid  = int(float(parts[0]))\n            tid  = int(float(parts[1]))\n            bbox = [float(parts[2]), float(parts[3]),\n                    float(parts[4]), float(parts[5])]\n            refined_by_frame[fid].append((tid, bbox))\n\n    # ── 2. Build passthrough (classes NOT refined) ───────────\n    passthrough_by_frame = defaultdict(list)  # frame_id -> [(tid, bbox, class_id)]\n    orig_by_frame        = defaultdict(list)   # frame_id -> [(tid, bbox, class_id)] — ALL original\n    all_orig_tids        = set()\n    for frame_id, tracks in all_tracks:\n        for t in tracks:\n            tlwh = t.tlwh.tolist()\n            orig_by_frame[frame_id].append((t.track_id, tlwh, t.class_id))\n            all_orig_tids.add(t.track_id)\n            if t.class_id not in GTA_CLASSES_TO_REFINE_LIST:\n                passthrough_by_frame[frame_id].append((t.track_id, tlwh, t.class_id))\n\n    # ── 3. Recover class labels for refined tracks via IoU ───\n    # Build original refined-class detections per frame for matching\n    orig_refined_by_frame = defaultdict(list)  # frame_id -> [(tid, bbox, class_id)]\n    for frame_id, tracks in all_tracks:\n        for t in tracks:\n            if t.class_id in GTA_CLASSES_TO_REFINE_LIST:\n                orig_refined_by_frame[frame_id].append((t.track_id, t.tlwh.tolist(), t.class_id))\n\n    refined_tid_class = {}  # refined_tid -> class_id (cached after first match)\n    for fid in sorted(refined_by_frame.keys()):\n        for (rtid, rbbox) in refined_by_frame[fid]:\n            if rtid in refined_tid_class:\n                continue\n            best_iou, best_class = 0.0, None\n            for (_otid, obbox, ocls) in orig_refined_by_frame.get(fid, []):\n                iou = _iou(rbbox, obbox)\n                if iou > best_iou:\n                    best_iou   = iou\n                    best_class = ocls\n            if best_iou > 0.1:\n                refined_tid_class[rtid] = best_class\n\n    print(f\"Class labels recovered for {len(refined_tid_class)} / {len(set(tid for dets in refined_by_frame.values() for tid, _ in dets))} refined track IDs\")\n\n    # ── 4. Remap refined track IDs to avoid collision ────────\n    id_offset   = max(all_orig_tids) + 1 if all_orig_tids else 1\n    refined_tids_sorted = sorted(set(tid for dets in refined_by_frame.values() for tid, _ in dets))\n    remap = {old: old + id_offset for old in refined_tids_sorted}\n    print(f\"Refined ID range remapped: [{min(refined_tids_sorted)}, {max(refined_tids_sorted)}] \"\n          f\"-> [{min(remap.values())}, {max(remap.values())}]\")\n\n    # ── 5. Write merged MOT .txt ──────────────────────────────\n    MERGED_MOT_PATH = \"/content/results_gta_merged.txt\"\n    merged_lines = []\n    # passthrough tracks — original IDs\n    for fid, dets in passthrough_by_frame.items():\n        for (tid, bbox, _cid) in dets:\n            # recover score from original\n            merged_lines.append(f\"{fid},{tid},{bbox[0]:.2f},{bbox[1]:.2f},{bbox[2]:.2f},{bbox[3]:.2f},1,-1,-1,-1\")\n    # refined tracks — remapped IDs\n    for fid, dets in refined_by_frame.items():\n        for (tid, bbox) in dets:\n            new_tid = remap[tid]\n            merged_lines.append(f\"{fid},{new_tid},{bbox[0]:.2f},{bbox[1]:.2f},{bbox[2]:.2f},{bbox[3]:.2f},1,-1,-1,-1\")\n    # sort by frame\n    merged_lines.sort(key=lambda l: (int(l.split(',')[0]), int(l.split(',')[1])))\n    with open(MERGED_MOT_PATH, \"w\") as f:\n        f.write(\"\\n\".join(merged_lines) + \"\\n\")\n    print(f\"Merged MOT .txt written to {MERGED_MOT_PATH}  ({len(merged_lines)} lines)\")\n\n    # ── 6. Re-render video ────────────────────────────────────\n    OUTPUT_GTA_VIDEO = \"/content/output_gta_refined.mp4\"\n    cap = cv2.VideoCapture(INPUT_VIDEO)\n    fps  = cap.get(cv2.CAP_PROP_FPS)\n    W    = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n    H    = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n    out = cv2.VideoWriter(OUTPUT_GTA_VIDEO, fourcc, fps, (W, H))\n\n    # index merged tracks by frame\n    merged_by_frame = defaultdict(list)  # frame_id -> [(tid, bbox, class_id, is_refined)]\n    for fid, dets in passthrough_by_frame.items():\n        for (tid, bbox, cid) in dets:\n            merged_by_frame[fid].append((tid, bbox, cid, False))\n    for fid, dets in refined_by_frame.items():\n        for (tid, bbox) in dets:\n            cid = refined_tid_class.get(tid, -1)\n            merged_by_frame[fid].append((remap[tid], bbox, cid, True))\n\n    from tqdm import tqdm as _tqdm\n    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n    for frame_idx in _tqdm(range(total_frames), desc=\"Re-rendering\"):\n        ret, frame = cap.read()\n        if not ret:\n            break\n        fid = frame_idx + 1\n        for (tid, bbox, cid, is_refined) in merged_by_frame.get(fid, []):\n            l, t, w, h = int(bbox[0]), int(bbox[1]), int(bbox[2]), int(bbox[3])\n            color = CLASS_COLORS_BGR.get(cid, (0, 255, 0))\n            cv2.rectangle(frame, (l, t), (l+w, t+h), color, 2)\n            cls_name = CLASS_NAMES[cid] if 0 <= cid < len(CLASS_NAMES) else f\"cls{cid}\"\n            label = f\"{cls_name} {tid}\" + (\" *\" if is_refined else \"\")\n            # label background\n            (tw, th), _ = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.55, 1)\n            cv2.rectangle(frame, (l, t - th - 2), (l + tw + 4, t), color, -1)\n            # text — black or white for contrast\n            b, g, r = color\n            lum = 0.299*r + 0.587*g + 0.114*b\n            txt_color = (0, 0, 0) if lum > 128 else (255, 255, 255)\n            cv2.putText(frame, label, (l + 2, t - 3),\n                        cv2.FONT_HERSHEY_SIMPLEX, 0.55, txt_color, 1)\n        out.write(frame)\n    cap.release()\n    out.release()\n    print(f\"Re-rendered video saved to {OUTPUT_GTA_VIDEO}\")\n\n    # ── 7. Summary table ──────────────────────────────────────\n    # Before: count unique track_ids per class from all_tracks\n    before_counts = defaultdict(set)  # class_id -> set of track_ids\n    for frame_id, tracks in all_tracks:\n        for t in tracks:\n            before_counts[t.class_id].add(t.track_id)\n\n    # After (refined classes): unique remapped IDs; passthrough classes unchanged\n    after_refined_tids = defaultdict(set)\n    for tid in refined_tids_sorted:\n        cid = refined_tid_class.get(tid, -1)\n        if cid >= 0:\n            after_refined_tids[cid].add(remap[tid])\n\n    print(\"\\n\" + \"=\" * 60)\n    print(\" GTA-Link Refinement Summary\")\n    print(\"=\" * 60)\n    print(f\"{'Class':<14} {'Before':>8} {'After':>8} {'Delta':>7}  Tag\")\n    print(\"-\" * 60)\n    for cid in sorted(set(list(before_counts.keys()) + list(after_refined_tids.keys()))):\n        name = CLASS_NAMES[cid] if 0 <= cid < len(CLASS_NAMES) else f\"class_{cid}\"\n        before_n = len(before_counts.get(cid, set()))\n        if cid in GTA_CLASSES_TO_REFINE_LIST:\n            after_n = len(after_refined_tids.get(cid, set()))\n            tag = \"(refined)\"\n        else:\n            after_n = before_n\n            tag = \"(passthrough)\"\n        delta = after_n - before_n\n        sign  = \"+\" if delta > 0 else \"\"\n        print(f\"{name:<14} {before_n:>8} {after_n:>8} {sign}{delta:>6}  {tag}\")\n    print(\"=\" * 60)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 8: Compress and Display Output Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Compress video for notebook display\n",
    "OUTPUT_COMPRESSED = \"/content/output_compressed.mp4\"\n",
    "!ffmpeg -y -i {config.output_video_path} -vcodec libx264 -crf 28 {OUTPUT_COMPRESSED}\n",
    "\n",
    "print(f\"Compressed video saved to: {OUTPUT_COMPRESSED}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "from base64 import b64encode\n",
    "\n",
    "OUTPUT_COMPRESSED = \"/content/output_compressed.mp4\"\n",
    "\n",
    "# Read and encode video\n",
    "mp4 = open(OUTPUT_COMPRESSED, 'rb').read()\n",
    "data_url = f\"data:video/mp4;base64,{b64encode(mp4).decode()}\"\n",
    "\n",
    "# Display video\n",
    "HTML(f'''\n",
    "<video width=\"800\" controls>\n",
    "  <source src=\"{data_url}\" type=\"video/mp4\">\n",
    "</video>\n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 9: Frame-by-Frame Processing (Optional)\n",
    "\n",
    "For more control, process frames individually using the generator API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Process frame-by-frame with generator and access mask info\n",
    "# Uncomment to run\n",
    "\n",
    "# from bananatracker import BananaTrackerPipeline\n",
    "# \n",
    "# pipeline = BananaTrackerPipeline(config)\n",
    "# \n",
    "# for frame_id, frame, tracks, vis_frame in pipeline.process_video_generator(INPUT_VIDEO):\n",
    "#     # Get track info as dictionaries\n",
    "#     track_info = pipeline.get_track_info(tracks)\n",
    "#     \n",
    "#     # Access mask data\n",
    "#     prediction_mask = pipeline.prediction_mask\n",
    "#     tracklet_mask_dict = pipeline.tracklet_mask_dict\n",
    "#     \n",
    "#     # Process each track\n",
    "#     for info in track_info:\n",
    "#         track_id = info['track_id']\n",
    "#         mask_id = tracklet_mask_dict.get(track_id, None)\n",
    "#         print(f\"Frame {frame_id}: Track {track_id} - {info['class_name']} - Mask ID: {mask_id}\")\n",
    "#     \n",
    "#     # Stop after 10 frames for demo\n",
    "#     if frame_id >= 10:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 10: Disable Masks (Performance Mode)\n",
    "\n",
    "If you need faster processing without mask enhancement, you can disable the mask module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create config without masks for faster processing\n# Uses the same optimized tracking parameters\nconfig_fast = BananaTrackerConfig(\n    yolo_weights=\"/content/HockeyAI_model_weight.pt\",\n    class_names=[\"Center Ice\", \"Faceoff\", \"Goalpost\", \"Goaltender\", \"Player\", \"Puck\", \"Referee\"],\n    track_classes=[3, 4, 5, 6],\n    special_classes=[5],\n    detection_conf_thresh=0.4,    # Lowered to catch more objects\n    \n    # Tracker settings - Optimized for stability\n    track_thresh=0.5,    # Lowered for more first-pass matches\n    track_buffer=90,     # 3 seconds at 30fps for better occlusion handling\n    cmc_method=\"orb\",\n    \n    # DISABLE mask module for speed\n    enable_masks=False,\n    \n    # Output\n    output_video_path=\"/content/output_fast.mp4\",\n    device=\"cuda:0\",\n)\n\n# pipeline_fast = BananaTrackerPipeline(config_fast)\n# all_tracks_fast = pipeline_fast.process_video(INPUT_VIDEO)\n\nprint(\"Fast mode config created (masks disabled)\")\nprint(f\"  - detection_conf_thresh: {config_fast.detection_conf_thresh}\")\nprint(f\"  - track_thresh: {config_fast.track_thresh}\")\nprint(f\"  - track_buffer: {config_fast.track_buffer} frames\")\nprint(\"Uncomment the lines above to run without mask enhancement\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture Overview\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────────────────────┐\n",
    "│                           BananaTracker Pipeline                             │\n",
    "├─────────────────────────────────────────────────────────────────────────────┤\n",
    "│                                                                              │\n",
    "│  Input Frame                                                                 │\n",
    "│      │                                                                       │\n",
    "│      ▼                                                                       │\n",
    "│  ┌─────────┐                                                                 │\n",
    "│  │ YOLOv8  │ ─────────────────────────────────────────────────────┐         │\n",
    "│  │Detector │                                                       │         │\n",
    "│  └────┬────┘                                                       │         │\n",
    "│       │ Detections [x1,y1,x2,y2,conf,class]                       │         │\n",
    "│       ▼                                                            │         │\n",
    "│  ┌──────────────┐                                                  │         │\n",
    "│  │ BananaTracker│◄─────────── Mask-Enhanced ───────────────────┐   │         │\n",
    "│  │  (ByteTrack) │             Cost Matrix                      │   │         │\n",
    "│  └──────┬───────┘                                              │   │         │\n",
    "│         │ Tracks, New Tracks, Removed IDs                      │   │         │\n",
    "│         ▼                                                      │   │         │\n",
    "│  ┌──────────────────────────────────────────────────────────┐  │   │         │\n",
    "│  │                    MaskManager                            │  │   │         │\n",
    "│  │  ┌───────────┐         ┌─────────┐                       │  │   │         │\n",
    "│  │  │  SAM2.1   │─────────│  Cutie  │                       │  │   │         │\n",
    "│  │  │ (Initial  │  Seed   │(Temporal│                       │  │   │         │\n",
    "│  │  │  Masks)   │  Masks  │ Propag.)│                       │  │   │         │\n",
    "│  │  └───────────┘         └────┬────┘                       │  │   │         │\n",
    "│  │                             │                             │  │   │         │\n",
    "│  │                     prediction_mask                       │──┘   │         │\n",
    "│  │                    tracklet_mask_dict                     │      │         │\n",
    "│  │                    mask_avg_prob_dict                     │      │         │\n",
    "│  └──────────────────────────────────────────────────────────┘      │         │\n",
    "│                                                                     │         │\n",
    "│  ┌─────────────┐                                                   │         │\n",
    "│  │ Visualizer  │◄──────────────────────────────────────────────────┘         │\n",
    "│  │ (with mask  │                                                             │\n",
    "│  │  overlay)   │                                                             │\n",
    "│  └──────┬──────┘                                                             │\n",
    "│         │                                                                    │\n",
    "│         ▼                                                                    │\n",
    "│    Output Frame                                                              │\n",
    "│                                                                              │\n",
    "└─────────────────────────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "### Key Components:\n",
    "\n",
    "1. **SAM2.1**: Called ONCE per new tracklet to create pixel-precise mask from bounding box\n",
    "2. **Cutie**: Called EVERY frame to propagate masks temporally\n",
    "3. **Mask-Enhanced Cost Matrix**: Uses `mc` (mask coverage) and `mf` (mask fill) metrics to improve association when IoU is ambiguous"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}