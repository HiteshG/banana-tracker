{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BananaTracker - Multi-Object Tracking with SAM2.1 Segmentation\n",
    "\n",
    "This notebook demonstrates how to use BananaTracker for multi-object tracking with:\n",
    "- **YOLOv8** detection\n",
    "- **ByteTrack-based** tracking\n",
    "- **SAM2.1** mask generation and temporal propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 1: Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install ultralytics opencv-python-headless tqdm\n",
    "!pip install lap cython_bbox  # For ByteTrack tracker core\n",
    "\n",
    "# SAM2.1 dependencies\n",
    "!pip install hydra-core omegaconf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 2: Clone Repositories and Install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the BananaTracker repository\n",
    "!git clone https://github.com/USER/bananatracker.git\n",
    "%cd bananatracker\n",
    "\n",
    "# Install in development mode\n",
    "!pip install -e .\n",
    "\n",
    "# Clone SAM2.1 real-time repository\n",
    "%cd ..\n",
    "!git clone https://github.com/USER/segment-anything-2-real-time.git\n",
    "%cd segment-anything-2-real-time\n",
    "\n",
    "# Download SAM2.1 checkpoints\n",
    "%cd checkpoints\n",
    "!bash download_ckpts.sh\n",
    "%cd ../.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 3: Configuration\n",
    "\n",
    "Configure the tracker with your model weights and SAM2.1 settings.\n",
    "\n",
    "### SAM2.1 Model Options:\n",
    "| Model | Checkpoint | Config | Quality | Speed |\n",
    "|-------|------------|--------|---------|-------|\n",
    "| Large (Recommended) | `sam2.1_hiera_large.pt` | `sam2.1_hiera_l.yaml` | Best | Slower |\n",
    "| Base+ | `sam2.1_hiera_base_plus.pt` | `sam2.1_hiera_b+.yaml` | Good | Medium |\n",
    "| Small | `sam2.1_hiera_small.pt` | `sam2.1_hiera_s.yaml` | Fair | Fast |\n",
    "| Tiny | `sam2.1_hiera_tiny.pt` | `sam2.1_hiera_t.yaml` | Basic | Fastest |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bananatracker import BananaTrackerConfig\n",
    "\n",
    "# =============================================================================\n",
    "# SAM2.1 MODEL CONFIGURATION - Modify these to change the segmentation model\n",
    "# =============================================================================\n",
    "SAM2_CHECKPOINT = \"checkpoints/sam2.1_hiera_large.pt\"  # Path to SAM2.1 checkpoint\n",
    "SAM2_CONFIG = \"configs/sam2.1/sam2.1_hiera_l.yaml\"     # Path to SAM2.1 config\n",
    "SAM2_REPO_PATH = \"/content/segment-anything-2-real-time\"  # Path to SAM2.1 repo\n",
    "\n",
    "# =============================================================================\n",
    "# TRACKER CONFIGURATION\n",
    "# =============================================================================\n",
    "config = BananaTrackerConfig(\n",
    "    # Detection\n",
    "    yolo_weights=\"/content/HockeyAI_model_weight.pt\",  # Update with your model path\n",
    "    class_names=[\"Center Ice\", \"Faceoff\", \"Goalpost\", \"Goaltender\", \"Player\", \"Puck\", \"Referee\"],\n",
    "    track_classes=[3, 4, 5, 6],  # Goaltender, Player, Puck, Referee\n",
    "    special_classes=[5],          # Puck - max-conf only\n",
    "    detection_conf_thresh=0.5,    # General confidence threshold\n",
    "    detection_iou_thresh=0.7,     # IoU threshold for YOLO NMS\n",
    "\n",
    "    # Post-processing: Centroid-based deduplication\n",
    "    centroid_dedup_enabled=True,     # Remove duplicate boxes for same player\n",
    "    centroid_dedup_max_distance=36,  # Max pixel distance to consider duplicates\n",
    "\n",
    "    # Tracker\n",
    "    track_thresh=0.6,\n",
    "    track_buffer=30,\n",
    "    cmc_method=\"orb\",  # Options: \"orb\", \"ecc\", \"sift\", \"sparseOptFlow\", \"none\"\n",
    "\n",
    "    # SAM2.1 Mask Propagation\n",
    "    sam2_enabled=True,                    # Enable mask generation with SAM2.1\n",
    "    sam2_checkpoint=SAM2_CHECKPOINT,      # SAM2.1 model checkpoint\n",
    "    sam2_config=SAM2_CONFIG,              # SAM2.1 config yaml\n",
    "    sam2_repo_path=SAM2_REPO_PATH,        # Path to segment-anything-2-real-time repo\n",
    "    mask_start_frame=1,                   # Frame to start mask creation (1-indexed)\n",
    "    mask_overlap_threshold=0.6,           # Skip mask creation for heavily overlapping bboxes\n",
    "    mask_alpha=0.4,                       # Mask overlay transparency for visualization\n",
    "\n",
    "    # Visualization\n",
    "    class_colors={\n",
    "        \"Goaltender\": (255, 165, 0),   # Orange\n",
    "        \"Player\": (255, 0, 0),          # Blue (BGR)\n",
    "        \"Puck\": (0, 255, 0),            # Green\n",
    "        \"Referee\": (0, 0, 255),         # Red\n",
    "    },\n",
    "    show_track_id=True,\n",
    "    line_thickness=2,\n",
    "\n",
    "    # Output\n",
    "    output_video_path=\"/content/output_tracked.mp4\",\n",
    "    output_txt_path=\"/content/results.txt\",\n",
    "    device=\"cuda:0\",\n",
    ")\n",
    "\n",
    "print(\"Configuration created!\")\n",
    "print(f\"\\nDetection:\")\n",
    "print(f\"  - Confidence threshold: {config.detection_conf_thresh}\")\n",
    "print(f\"  - IoU threshold: {config.detection_iou_thresh}\")\n",
    "print(f\"  - Tracking classes: {config.track_classes}\")\n",
    "print(f\"  - Special classes (max-conf only): {config.special_classes}\")\n",
    "print(f\"\\nSAM2.1 Mask Propagation:\")\n",
    "print(f\"  - Enabled: {config.sam2_enabled}\")\n",
    "print(f\"  - Checkpoint: {config.sam2_checkpoint}\")\n",
    "print(f\"  - Config: {config.sam2_config}\")\n",
    "print(f\"  - Mask alpha: {config.mask_alpha}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 4: Load Model and Setup Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bananatracker import BananaTrackerPipeline\n",
    "\n",
    "# Initialize the pipeline\n",
    "pipeline = BananaTrackerPipeline(config)\n",
    "\n",
    "print(\"Pipeline initialized!\")\n",
    "print(f\"Detector: YOLOv8\")\n",
    "print(f\"Tracker: BananaTracker (ByteTrack-based)\")\n",
    "print(f\"Mask Manager: SAM2.1 Camera Predictor\")\n",
    "print(f\"CMC Method: {config.cmc_method}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 5: Run Tracking with Mask Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to input video\n",
    "INPUT_VIDEO = \"/content/sample_video.mp4\"  # Update with your video path\n",
    "\n",
    "# Run tracking with SAM2.1 mask propagation\n",
    "print(f\"Processing video: {INPUT_VIDEO}\")\n",
    "print(f\"SAM2.1 mask propagation: {'Enabled' if config.sam2_enabled else 'Disabled'}\")\n",
    "\n",
    "all_tracks = pipeline.process_video(INPUT_VIDEO)\n",
    "\n",
    "print(f\"\\nProcessed {len(all_tracks)} frames\")\n",
    "print(f\"Output video: {config.output_video_path}\")\n",
    "print(f\"MOT results: {config.output_txt_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 6: Validation - Show Mask on Random Frame\n",
    "\n",
    "This cell validates the SAM2.1 integration by displaying masks on a random frame from the video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import random\n",
    "from IPython.display import display, Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Reload video and process a random frame with masks\n",
    "cap = cv2.VideoCapture(INPUT_VIDEO)\n",
    "total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "# Select a random frame (after mask initialization)\n",
    "random_frame_idx = random.randint(max(10, config.mask_start_frame + 5), min(total_frames - 1, 100))\n",
    "print(f\"Selected random frame: {random_frame_idx}\")\n",
    "\n",
    "# Reset pipeline for fresh processing\n",
    "pipeline.tracker.reset()\n",
    "if pipeline.mask_manager:\n",
    "    pipeline.mask_manager.reset()\n",
    "\n",
    "# Process frames up to random frame\n",
    "prev_frame = None\n",
    "frame_id = 0\n",
    "result_frame = None\n",
    "result_mask = None\n",
    "\n",
    "while frame_id <= random_frame_idx:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    frame_id += 1\n",
    "    \n",
    "    # Detect\n",
    "    detections = pipeline.detector.detect(frame)\n",
    "    \n",
    "    # Track\n",
    "    height, width = frame.shape[:2]\n",
    "    tracks, removed_ids, new_tracks = pipeline.tracker.update(\n",
    "        detections_array=detections,\n",
    "        img_info=(height, width),\n",
    "        frame_img=frame\n",
    "    )\n",
    "    \n",
    "    # Update masks\n",
    "    mask = None\n",
    "    tracklet_mask_dict = {}\n",
    "    \n",
    "    if pipeline.mask_manager:\n",
    "        online_tlwhs = [t.tlwh.tolist() for t in tracks]\n",
    "        online_ids = [t.track_id for t in tracks]\n",
    "        \n",
    "        mask, tracklet_mask_dict, _, mask_colors = (\n",
    "            pipeline.mask_manager.get_updated_masks(\n",
    "                frame=frame,\n",
    "                frame_prev=prev_frame,\n",
    "                frame_id=frame_id,\n",
    "                online_tlwhs=online_tlwhs,\n",
    "                online_ids=online_ids,\n",
    "                new_tracks=new_tracks,\n",
    "                removed_tracks_ids=removed_ids,\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        if mask_colors is not None:\n",
    "            mask = mask_colors\n",
    "    \n",
    "    # Store for visualization\n",
    "    if frame_id == random_frame_idx:\n",
    "        result_frame = frame.copy()\n",
    "        result_mask = mask\n",
    "        result_tracks = tracks\n",
    "        result_tracklet_mask_dict = tracklet_mask_dict\n",
    "    \n",
    "    prev_frame = frame.copy()\n",
    "\n",
    "cap.release()\n",
    "\n",
    "# Visualize results\n",
    "if result_frame is not None:\n",
    "    # Create visualization\n",
    "    vis_frame = pipeline.visualizer.draw_tracks_with_masks(\n",
    "        result_frame, result_tracks, result_mask, result_tracklet_mask_dict\n",
    "    )\n",
    "    \n",
    "    # Display\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "    \n",
    "    # Original frame\n",
    "    axes[0].imshow(cv2.cvtColor(result_frame, cv2.COLOR_BGR2RGB))\n",
    "    axes[0].set_title(f\"Original Frame {random_frame_idx}\")\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    # Mask only\n",
    "    if result_mask is not None:\n",
    "        axes[1].imshow(result_mask, cmap='nipy_spectral')\n",
    "        axes[1].set_title(f\"SAM2.1 Mask ({len(np.unique(result_mask)) - 1} objects)\")\n",
    "    else:\n",
    "        axes[1].text(0.5, 0.5, 'No mask available', ha='center', va='center')\n",
    "        axes[1].set_title(\"SAM2.1 Mask\")\n",
    "    axes[1].axis('off')\n",
    "    \n",
    "    # Combined visualization\n",
    "    axes[2].imshow(cv2.cvtColor(vis_frame, cv2.COLOR_BGR2RGB))\n",
    "    axes[2].set_title(f\"Tracking + Masks ({len(result_tracks)} tracks)\")\n",
    "    axes[2].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print statistics\n",
    "    print(f\"\\nFrame {random_frame_idx} Statistics:\")\n",
    "    print(f\"  - Active tracks: {len(result_tracks)}\")\n",
    "    print(f\"  - Track IDs: {[t.track_id for t in result_tracks]}\")\n",
    "    if result_mask is not None:\n",
    "        unique_masks = np.unique(result_mask)\n",
    "        print(f\"  - Unique mask IDs: {unique_masks.tolist()}\")\n",
    "        print(f\"  - Tracklet-to-mask mapping: {result_tracklet_mask_dict}\")\n",
    "else:\n",
    "    print(\"Could not process frame\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 7: Compress Output Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Compress video for notebook display\n",
    "OUTPUT_COMPRESSED = \"/content/output_compressed.mp4\"\n",
    "!ffmpeg -y -i {config.output_video_path} -vcodec libx264 -crf 28 {OUTPUT_COMPRESSED}\n",
    "\n",
    "print(f\"Compressed video saved to: {OUTPUT_COMPRESSED}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 8: Display Video in Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "from base64 import b64encode\n",
    "\n",
    "OUTPUT_COMPRESSED = \"/content/output_compressed.mp4\"\n",
    "\n",
    "# Read and encode video\n",
    "mp4 = open(OUTPUT_COMPRESSED, 'rb').read()\n",
    "data_url = f\"data:video/mp4;base64,{b64encode(mp4).decode()}\"\n",
    "\n",
    "# Display video\n",
    "HTML(f'''\n",
    "<video width=\"800\" controls>\n",
    "  <source src=\"{data_url}\" type=\"video/mp4\">\n",
    "</video>\n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional: Frame-by-Frame Processing with Masks\n",
    "\n",
    "For more control, you can process frames individually using the generator API which now includes mask data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Process frame-by-frame with generator\n",
    "# Uncomment to run\n",
    "\n",
    "# from bananatracker import BananaTrackerPipeline\n",
    "# \n",
    "# pipeline = BananaTrackerPipeline(config)\n",
    "# \n",
    "# for frame_id, frame, tracks, vis_frame, mask, tracklet_mask_dict in pipeline.process_video_generator(INPUT_VIDEO):\n",
    "#     # Get track info as dictionaries\n",
    "#     track_info = pipeline.get_track_info(tracks)\n",
    "#     \n",
    "#     # Process each track\n",
    "#     for info in track_info:\n",
    "#         print(f\"Frame {frame_id}: Track {info['track_id']} - {info['class_name']} at {info['bbox']}\")\n",
    "#     \n",
    "#     # Access mask data\n",
    "#     if mask is not None:\n",
    "#         print(f\"  Mask has {len(np.unique(mask)) - 1} segmented objects\")\n",
    "#     \n",
    "#     # Stop after 10 frames for demo\n",
    "#     if frame_id >= 10:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration Reference\n",
    "\n",
    "### SAM2.1 Model Options\n",
    "\n",
    "```python\n",
    "# Large model (best quality, recommended for accuracy)\n",
    "SAM2_CHECKPOINT = \"checkpoints/sam2.1_hiera_large.pt\"\n",
    "SAM2_CONFIG = \"configs/sam2.1/sam2.1_hiera_l.yaml\"\n",
    "\n",
    "# Base+ model (balanced quality/speed)\n",
    "SAM2_CHECKPOINT = \"checkpoints/sam2.1_hiera_base_plus.pt\"\n",
    "SAM2_CONFIG = \"configs/sam2.1/sam2.1_hiera_b+.yaml\"\n",
    "\n",
    "# Small model (faster, lower quality)\n",
    "SAM2_CHECKPOINT = \"checkpoints/sam2.1_hiera_small.pt\"\n",
    "SAM2_CONFIG = \"configs/sam2.1/sam2.1_hiera_s.yaml\"\n",
    "\n",
    "# Tiny model (fastest, basic quality)\n",
    "SAM2_CHECKPOINT = \"checkpoints/sam2.1_hiera_tiny.pt\"\n",
    "SAM2_CONFIG = \"configs/sam2.1/sam2.1_hiera_t.yaml\"\n",
    "```\n",
    "\n",
    "### Key Configuration Options\n",
    "\n",
    "| Parameter | Description | Default |\n",
    "|-----------|-------------|--------|\n",
    "| `sam2_enabled` | Enable/disable SAM2.1 mask propagation | `True` |\n",
    "| `mask_start_frame` | Frame to start mask creation | `1` |\n",
    "| `mask_overlap_threshold` | Skip masks for heavily overlapping bboxes | `0.6` |\n",
    "| `mask_alpha` | Mask overlay transparency | `0.4` |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
